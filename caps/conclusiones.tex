%!TEX root = ../main.tex

En esta tesis nos preguntamos cómo replicar la habilidad humana para construir modelos estructurados de conocimiento abstracto a partir de pocos datos. Exploramos distintos modelos probabilísticos de \lot para explicar y predecir el comportamiento humano en nuevos dominios. Los modelos de \lot utilizados tienen el poder de expresividad de las gramáticas para expresar infinitos conceptos a partir de pocas reglas, y la flexibilidad de los modelos probabilísticos para explicar distintas trayectorias de aprendizaje de conceptos. A su vez, hemos desarrollados distintos métodos y marcos novedosos para validarlos y ponerlos a prueba en diferentes escenarios.

En el capítulo~\ref{chapter:BIN}, \textit{una teoría de la memoria para secuencias binarias: evidencia de un algoritmo de compresión mental en humanos~\cite{planton2021memory}}, pusimos a prueba la teoría de que los adultos humanos codifican secuencias binarias de estímulos en la memoria utilizando un \lot y un algoritmo de compresión recursivo a través de cinco experimentos compuestos por una variedad de secuencias auditivas y visuales. Proporcionamos una primera demostración de que, incluso después de tener en cuenta las probabilidades de transición, las respuestas a las violaciones de secuencia se pueden utilizar para descubrir las propiedades del lenguaje abstracto del pensamiento utilizado por los individuos para codificar patrones secuenciales. La propuesta de \grambin, que toma la forma de un lenguaje formal psicológicamente plausible compuesto por un conjunto restringido de reglas simples, demostró ser más eficaz que los enfoques alternativos en el modelado de la memoria humana para secuencias simples. La relación observada entre la complejidad de la secuencia, \mdlbin, y el desempeño en la detección de violaciones es coherente con la idea de que el cerebro actúa como un compresor de información entrante que captura regularidades y las usa para predecir el resto de la secuencia. El presente paradigma pasivo y no verbal allana el camino para futuros estudios de registros neurofisiológicos que podrían probar las similitudes y diferencias entre los humanos y otras especies~\cite{f13} o probar las habilidades de los bebés preverbales~\cite{f70}. Una pregunta fundamental para las investigaciones futuras es si el mismo lenguaje formal puede explicar el procesamiento de secuencias en otras especies de primates, o si tal lenguaje es exclusivo de los humanos~\cite{f6}.

Hemos discutido también en ese mismo capítulo si los resultados podrían estar sesgados dado que comenzamos con un \lot preconcebido y secuencias seleccionadas cuyas estructuras fueron bien capturadas por ese lenguaje (así como algunas secuencias que eran máximamente irregulares también según ese lenguaje). Si bien hemos planteado algunos argumentos que permiten descartar esa idea, profundizamos sobre este problema en el capítulo~\ref{chapter:PO}, \textit{validación bayesiana de producciones gramaticales para el \lot~\cite{romano2018bayesian}}, donde presentaremos un método de validación para los modelos \lot que permite tener mayores certezas sobre las reglas atómicas elegidas para construir los lenguajes. Aplicamos este método en el lenguaje \gramgeo, el \lot de secuencias geométricas sobre el que se basó \grambin, para distinguir entre las producciones originales y un conjunto de producciones ad-hoc arbitrarias. Finalmente, a pesar de que \gramgeo no es un lenguaje universal, mostramos una relación empírica entre la probabilidad de una secuencia y su complejidad, consistente con la relación teórica para los lenguajes universales descrita por el Teorema de codificación de Levin~\cite{levin1974laws}. Esto abre una oportunidad para cerrar la brecha entre los enfoques de \lot basados en métodos de longitud mínima de descripción y aquellos basados en modelos probabilísticos bayesianos.

En ambos trabajos de la parte~\ref{parte:secuencias} (así como en la mayoría de los estudios sobre \lot) un modelo fijo fue elegido y calibrado a partir de los datos, lo cual genera cierta inconsistencia con la extensa literatura que muestra cómo la experiencia da forma permanente al aprendizaje de conceptos. Por eso en los trabajos de la parte \ref{parte:conceptos} nos focalizamos en estudiar los cambios del \lot a lo largo de una secuencia de conceptos. En el capítulo~\ref{chapter:PRE}, \textit{hacia un \lot más flexible: la gramática Bayesiana se actualiza después de cada exposición de conceptos~\cite{tano2020towards}}, realizamos un experimento para investigar la plasticidad del \lot. Definimos un modelo para medir la dificultad subjetiva de aprender una secuencia de conceptos lógicos. El modelo actualiza las probabilidades de producción gramaticales entre conceptos y predice la dificultad como el tamaño de las fórmulas compatibles ponderado por su probabilidad a posteriori. Este mecanismo de aprendizaje permite simular la aparición de una nueva primitiva en el lenguaje, ya que resulta útil para codificar los conceptos presentados hasta ese momento. Las dificultades predichas se asemejan mucho al patrón de los tiempos de aprendizaje humano en una secuencia de conceptos que requirieron el operador del o-exclusivo lógico $ \oxor $ para ser representados de manera eficiente.

Finalmente, en el capítulo~\ref{chapter:BRM}, \textit{un marco lógico para estudiar los sesgos de aprendizaje de conceptos en presencia de múltiples explicaciones~\cite{tano2021framework}}, ampliamos el diseño experimental del capítulo anterior para desarrollar un marco experimental novedoso que permite presentar conceptos que son simultáneamente consistentes con dos o más reglas de diferente complejidad y que utilizan diferentes características para comparar, de forma directa y bajo las mismas condiciones experimentales, los sesgos de aprendizaje de conceptos. Explotamos el marco estudiado en un experimento con una secuencia de conceptos lógicos para ilustrar la aparición de sesgos conocidos y estudiados previamente, y de otros sesgos y efectos de transferencia caracterizados por primer vez. Dejamos para un trabajo futuro la tarea de estudiar la interacción entre el sesgo de adherencia de características y la estructura precisa de las reglas lógicas que se están aprendiendo, como así también explorar en mayor detalle la dificultad relativa de las reglas con MDL ligeramente diferentes en el marco de múltiples explicaciones consistentes.

Mucho se ha avanzado en los últimos años para entender cómo funciona y se desarrolla la inteligencia humana, pero aún queda mucho camino por recorrer. Los modelos probabilísticos de \lot presentan la ventaja de contar con un formalismo matemático robusto para construir modelos de pensamiento con muy pocos parámetros libres y asunciones a priori. A su vez, permiten la inferencia estadística para explicar trayectorias dinámicas de aprendizaje a partir de datos escasos, salteando la vieja dicotomía entre los modelos simbólicos y estadísticos.

Sin embargo, para expresar todas las posibles teorías de la mente humana se necesitan de modelos \lot Turing completos, los cuales presentan desafíos computacionales para poder explorar de manera efectiva el dominio de todas las hipótesis y modelar las trayectorias de aprendizaje sin que se vuelva un problema intratable computacionalmente. La pregunta clave sigue siendo cómo la mente representa el conocimiento simbólico de tantos dominios e implementa estos algoritmos de inferencia en los circuitos neuronales. Si bien los conexionistas niegan que la mente codifique dicho conocimiento, este trabajo es un aporte más a la enorme cantidad de publicaciones que muestran que la construcción de modelos simbólicos son esenciales para el pensamiento humano.