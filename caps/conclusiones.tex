%!TEX root = ../main.tex

\widesanti{está muy bien!!! Lo que no me gusta es la negrita para el nombre de los capítulos. Se podría usar itálica, o simplemente eliminar el nombre del capítulo.}
\widesergio{los puse en itálica, me gusta la idea de reforzar que todo capítulo es también una publicación}

Párrafo inicial retomando intro

En el capítulo~\ref{chapter:BIN}, \textit{una teoría de la memoria para secuencias binarias: evidencia de un algoritmo de compresión mental en humanos~\cite{planton2021memory}}, pusimos a prueba la teoría de que los adultos humanos codifican secuencias binarias de estímulos en la memoria utilizando un \lot y un algoritmo de compresión recursivo a través de cinco experimentos compuestos por una variedad de secuencias auditivas y visuales. Proporcionamos una primera demostración de que, incluso después de tener en cuenta las probabilidades de transición, las respuestas a las violaciones de secuencia se pueden utilizar para descubrir las propiedades del lenguaje abstracto del pensamiento utilizado por los individuos para codificar patrones secuenciales. La propuesta de \grambin, que toma la forma de un lenguaje formal psicológicamente plausible compuesto por un conjunto restringido de reglas simples, demostró ser más eficaz que los enfoques alternativos en el modelado de la memoria humana para secuencias simples. La relación observada entre la complejidad de la secuencia, \mdlbin, y el desempeño en la detección de violaciones es coherente con la idea de que el cerebro actúa como un compresor de información entrante que captura regularidades y las usa para predecir el resto de la secuencia. El presente paradigma pasivo y no verbal allana el camino para futuros estudios de registros neurofisiológicos que podrían probar las similitudes y diferencias entre los humanos y otras especies~\cite{f13} o probar las habilidades de los bebés preverbales~\cite{f70}. Una pregunta fundamental para las investigaciones futuras es si el mismo lenguaje formal puede explicar el procesamiento de secuencias en otras especies de primates, o si tal lenguaje es exclusivo de los humanos~\cite{f6}.

Hemos discutido también en ese mismo capítulo si los resultados podrían estar sesgados dado que comenzamos con un \lot preconcebido y secuencias seleccionadas cuyas estructuras fueron bien capturadas por ese lenguaje (así como algunas secuencias que eran máximamente irregulares también según ese lenguaje). Si bien hemos planteado algunos argumentos que permiten descartar esa idea, profundizamos sobre este problema en el capítulo~\ref{chapter:PO}, \textit{validación bayesiana de producciones gramaticales para el \lot~\cite{romano2018bayesian}}, donde presentaremos un método de validación para los modelos \lot que permite tener mayores certezas sobre las reglas atómicas elegidas para construir los lenguajes. Aplicamos este método en el lenguaje \gramgeo, el \lot de secuencias geométricas sobre el que se basó \grambin, para distinguir entre las producciones originales y un conjunto de producciones ad-hoc arbitrarias. Finalmente, a pesar de que \gramgeo no es un lenguaje universal, mostramos una relación empírica entre la probabilidad de una secuencia y su complejidad, consistente con la relación teórica para los lenguajes universales descrita por el Teorema de codificación de Levin~\cite{levin1974laws}. Esto abre una oportunidad para cerrar la brecha entre los enfoques de \lot basados en métodos de longitud mínima de descripción y aquellos basados en modelos probabilísticos bayesianos.

En ambos trabajos de la parte~\ref{parte:secuencias} (así como en la mayoría de los estudios sobre \lot) un modelo fijo es elegido y calibrado a partir de los datos, lo cual genera cierta inconsistencia con la extensa literatura que muestra cómo la experiencia da forma permanente al aprendizaje de conceptos. Por eso en los trabajos de la parte \ref{parte:conceptos} nos focalizamos en estudiar los cambios del \lot a lo largo de una secuencia de conceptos. En el capítulo~\ref{chapter:PRE}, \textit{hacia un \lot más flexible: la gramática Bayesiana se actualiza después de cada exposición de conceptos~\cite{tano2020towards}}, realizamos un experimento para investigar la plasticidad del \lot. Definimos un modelo para medir la dificultad subjetiva de aprender una secuencia de conceptos lógicos. El modelo actualiza las probabilidades de producción gramaticales entre conceptos y predice la dificultad como el tamaño de las fórmulas compatibles ponderado por su probabilidad a posteriori. Este mecanismo de aprendizaje permite simular la aparición de una nueva primitiva en el lenguaje, ya que resulta útil para codificar los conceptos presentados hasta ese momento. Las dificultades predichas se asemejan mucho al patrón de los tiempos de aprendizaje humano en una secuencia de conceptos que requirieron el operador del o-exclusivo lógico $ \oxor $ para ser representados de manera eficiente.

Finalmente, en el capítulo~\ref{chapter:BRM}, \textit{un marco lógico para estudiar los sesgos de aprendizaje de conceptos en presencia de múltiples explicaciones~\cite{tano2021framework}}, ampliamos el diseño experimental del capítulo anterior para desarrollar un marco experimental novedoso que permite presentar conceptos que son simultáneamente consistentes con dos o más reglas de diferente complejidad y que utilizan diferentes características para comparar, de forma directa y bajo las mismas condiciones experimentales, los sesgos de aprendizaje de conceptos. Explotamos el marco estudiado en un experimento con una secuencia de conceptos lógicos para ilustrar la aparición de sesgos conocidos y estudiados previamente, y de otros sesgos y efectos de transferencia caracterizados por primer vez. 

Párrafo final cerrando.