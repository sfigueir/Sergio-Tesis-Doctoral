%!TEX root = ../main.tex





\chapter{Introducción}
\label{intro}

En las últimas dos décadas distintas técnicas de ingeniería reversa del aprendizaje en humanos han inspirado con éxito distintos algoritmos de inteligencia artificial \cite{russell2002artificial}. Los avances recientes en las técnicas de aprendizaje profundo han logrado resultados notables en numerosos dominios como reconocimiento visual de objetos, el reconocimiento automático del habla, la búsqueda de respuestas y las traducciones automáticas \cite{lecun2015deep}. En la mayoría de estos enfoques, el resultado y el objeto del proceso de aprendizaje es una función estadística de reconocimiento de patrones específicos en los datos. Sin embargo, en muchas situaciones, el aprendizaje humano implica la construcción de modelos estructurados de conocimiento abstracto a partir de pocos datos, y este tipo de sistemas no han sido capaces de imitar esa habilidad \cite{lake2017building}.

¿Cómo pueden las personas adquirir un vasto universo de conceptos con muy poca exposición aparente? Una posible solución a este enigma, conocida como el problema de Platón ~\cite{chomsky1986knowledge, chomsky2006cognitive}, surge del aprendizaje automático probabilístico. Este enfoque está arrojando algo de luz sobre cómo los humanos pueden construir modelos y abstracciones bajo incertidumbre y a partir de datos escasos \cite{tenenbaum2011grow,ghahramani2015probabilistic}, y está renovando la hipótesis de Jerry Fodor que afirma que el pensamiento toma forma en una especie de lenguaje mental del pensamiento (\lot, por sus siglas en inglés) compuesto por un conjunto limitado de símbolos atómicos que se pueden combinar para formar estructuras más complejas siguiendo reglas combinatorias \cite{fodor1975language}.

Nuestra investigación se suscribe a uno de las líneas actuales del aprendizaje automático probabilístico conocido como programación probabilística\sergio{Decidir si usar programación probabilística o mejor un concepto más utilizado en el resto de la tesis}, un esquema general para expresar modelos probabilísticos y métodos de inferencia como programas informáticos \cite{ghahramani2015probabilistic}. Esto significa que en nuestros modelos asumimos que el \lot es un lenguaje de programación capaz de generar programas para modelar conceptos en el mundo. Con nuestro trabajo pretendemos mejorar nuestro entendimiento del proceso de aprendizaje a partir de cuerpos ralos de datos y desarrollar nuevos métodos y algoritmos de programación probabilística para replicar esta notable capacidad humana. 


\subsection{Aportes de esta tesis}

\paragraph{Una teoría de la memoria para secuencias binarias: evidencia de un algoritmo de compresión mental en humanos~\cite{planton2021memory}.} 
La capacidad de la memoria de trabajo se puede mejorar recodificando la información memorizada en forma condensada. En el Capítulo~\ref{chapter:BIN}, ponemos a prueba la teoría de que los adultos humanos codifican secuencias binarias de estímulos en la memoria utilizando un \lot y un algoritmo de compresión recursivo. La teoría predice que la complejidad psicológica de una secuencia dada debería ser proporcional a la longitud de su descripción más corta en el lenguaje propuesto \grambin, que puede capturar cualquier patrón anidado de repeticiones y alternancias usando un número limitado de instrucciones. El lenguaje \grambin es una versión simplificada para el dominio de las secuencias binarias del lenguaje de geometría, \gramgeo, propuesto en~\cite{amalric2017language}. Cinco experimentos examinan la capacidad de la teoría para predecir la memoria de los adultos humanos para una variedad de secuencias auditivas y visuales. Pusimos a prueba la memoria utilizando un paradigma de violación de secuencia en el que los participantes intentaron detectar violaciones ocasionales en una secuencia que de otro modo sería fija. Tanto las calificaciones de complejidad subjetiva como el rendimiento de detección de violaciones objetivas fueron bien predichas por nuestra medida teórica de complejidad, que resulta ser la complejidad de Kolmogorov para el lenguaje específico de geometría en cadenas binarias, \grambin. Los resultados apoyan la hipótesis de que, más allá de la extracción de conocimiento estadístico, la codificación de secuencias humanas se basa en una compresión interna que utiliza estructuras anidadas similares a la de las Lenguaje del Pensamiento propuesto.


\paragraph{Validación Bayesiana de producciones gramaticales para el \lot~\cite{romano2018bayesian}.} 
Las propuestas probabilísticas del \lot pueden explicar el aprendizaje en diferentes dominios como una inferencia estadística sobre un espacio de hipótesis estructurado composicionalmente. Si bien los marcos pueden diferir en cómo se puede implementar un \lot computacionalmente, todos comparten la propiedad de que se construyen a partir de un conjunto de símbolos atómicos y reglas mediante las cuales estos símbolos se pueden combinar. En el Capítulo~\ref{chapter:PO} se propone un paso de validación extra para el conjunto de producciones atómicas definidas por el experimentador. Comienza expandiendo la gramática \lot definida para el dominio cognitivo con un conjunto más amplio de producciones arbitrarias y luego usa la inferencia Bayesiana para podar las producciones de los datos experimentales. El resultado permite al investigador validar que la gramática resultante aún coincide con la gramática intuitiva elegida para el dominio. Luego se prueba este método en el lenguaje \gramgeo, un modelo de \lot específico para el aprendizaje de secuencias geométricas propuesto en~\cite{amalric2017language}. Finalmente, a pesar de que \gramgeo no es un lenguaje universal (es decir, Turing completo), mostramos una relación empírica entre la probabilidad de una secuencia y su complejidad, consistente con la relación teórica para los lenguajes universales descrita por el Teorema de codificación de Levin~\cite{levin1974laws}.


\paragraph{Hacia un \lot más flexible: la gramática Bayesiana se actualiza después de cada exposición de conceptos~\cite{tano2020towards}.}
Los enfoques recientes del aprendizaje de conceptos humanos han combinado con éxito el poder de los sistemas de reglas simbólicos e infinitamente productivos y el aprendizaje estadístico para explicar nuestra capacidad de aprender nuevos conceptos a partir de unos pocos ejemplos. El objetivo de la mayoría de estos estudios es revelar el lenguaje subyacente que estructura estas representaciones y proporciona un sustrato general para el pensamiento. Sin embargo, describir un modelo de pensamiento que se fija una vez entrenado va en contra de la extensa literatura que muestra cómo la experiencia da forma al aprendizaje de conceptos. En el Capítulo~\ref{chapter:PRE} se investiga la plasticidad de estos lenguajes descriptivos simbólicos. Se realiza un experimento de aprendizaje de conceptos que demuestra que los seres humanos pueden cambiar muy rápidamente el repertorio de símbolos que utilizan para identificar conceptos, compilando expresiones que se utilizan con frecuencia en nuevos símbolos del lenguaje. El patrón de tiempos de aprendizaje de conceptos es descrito con precisión por un agente Bayesiano que actualiza racionalmente la probabilidad de compilar una nueva expresión de acuerdo con lo útil que ha sido para comprimir conceptos hasta ahora. Al presentar el \lot como un sistema flexible de reglas, también destacamos las dificultades para precisarlo empíricamente.

\paragraph{Un marco lógico para estudiar los sesgos de aprendizaje de conceptos en presencia de múltiples explicaciones~\cite{tano2021framework}.}
Cuando las personas buscan comprender conceptos a partir de un conjunto incompleto de ejemplos y contraejemplos, suele haber una cantidad exponencial de reglas de clasificación que pueden clasificar correctamente los datos observados, según las características de los ejemplos que se utilicen para construir estas reglas. Una aproximación mecanicista del aprendizaje de conceptos humanos debería ayudar a explicar cómo los humanos prefieren algunas reglas por sobre otras cuando hay muchas que pueden usarse para clasificar correctamente los datos observados. En el Capítulo~\ref{chapter:BRM}, se explotan las herramientas de la lógica proposicional para desarrollar un marco experimental --continuando y ampliando aquel desarrollado en el Capítulo~\ref{chapter:PRE}-- que controle las reglas mínimas que son \textit{simultáneamente} consistentes con los ejemplos presentados. Por ejemplo, este marco nos permite presentar a los participantes conceptos consistentes con una disyunción \textit{y también} con una conjunción, dependiendo de qué características se usen para construir la regla. Del mismo modo, nos permite presentar conceptos que son simultáneamente consistentes con dos o más reglas de diferente complejidad y que utilizan diferentes características. Es importante destacar que el marco lógico propuesto controla completamente qué reglas mínimas compiten para explicar los ejemplos y es capaz de recuperar las características utilizadas por el participante para construir la regla de clasificación, sin depender de mecanismos complementarios de seguimiento de la atención (por ejemplo, {\em eye-tracking}). Explotamos el marco estudiado en un experimento con una secuencia pruebas competitivas como las mencionadas, e ilustramos la aparición de varios efectos de transferencia que sesgan la atención previa de los participantes a conjuntos específicos de características durante el aprendizaje.




\section{Teoría computacional de la mente}
\textbf{Explicar critica conexionismo y surgimiento teoría computacional de la mente.} \\
\textbf{Breve mención a críticas del conectivismo a partir del 80 (Fodor y Steven Pinker). Reversión al asociacionismo.}\\
\textbf{Explicar Simbólico}\\

"The infite use of finite means" (Humboldt's sobre el lenguaje)

1) How does abstract knowledge guide learning and inference from sparse data?
Bayesian inference in probabilistic generative models

2) What form does that knowledge take, across different domains and tasks?
Probabilities defined over richly structured symbolic representations: spaces, graphs, grammars, logical predicates

3) How is that knowledge itself constructed / updated / validated?
Hierarchical models, transfer learning, herramientas papers


Los investigadores han modelado estas categorías mentales o clases conceptuales con dos enfoques clásicos: en términos de similitud con un ejemplo genérico o prototipo \cite{rosch1999principles, nosofsky1986attention, rosch1976structural, rosch1975family} o basándose en una representación simbólica de reglas \cite{boole1854investigation, fodor1975language, gentner1983structure}.

Enfoques simbólicos como la hipótesis del \textit{lenguaje del pensamiento} (LoT, por sus siglas en ingés) \cite{fodor1975language}, afirman que el pensamiento toma forma en una especie de lenguaje mental, compuesto por un conjunto limitado de símbolos atómicos que pueden combinarse para formar estructuras más complejas. siguiendo reglas combinatorias.

\intro{material para intro, pero dejar algo de relleno acá}{
%How can children acquire a vast universe of concepts with seemingly very little exposure? One possible solution to this conundrum, known as the Plato Problem~\cite{chomsky1986knowledge,chomsky2006cognitive}, builds on the human capacity to describe concepts --and more generally of all elements of thought-- through the use of a symbolic and combinatorial mental language~\cite{newell1980physical}, referred as {\em language of thought} (\lot)~\cite{fodor1975language}.

¿Cómo pueden los niños adquirir un vasto universo de conceptos con muy poca exposición aparente? Una posible solución a esta pregunta, conocida como el problema de Platón~\cite{chomsky1986knowledge,chomsky2006cognitive}, se basa en la capacidad humana para describir conceptos (y en general cualquier elemento del pensamiento) mediante el uso de un lenguaje mental con elementos simbólicos combinatorios~\cite{newell1980physical}, conocido en la literatura como el {\em lenguaje del pensamiento} (\lot, por sus siglas en inglés)~\cite{fodor1975language}.

%Combinatorial languages can describe a vast set of concepts from a small set of primitives. This can be understood in a relatively simple example in the domain of shapes. A combinatorial and symbolic language similar to Logo~\cite{abelson1974logo} can combine operations such as ``move", ``pen up", ``pen down" or ``rotate" to generate an infinite set of expressions (or programs) which, when evaluated, can convey all sort of shapes.

(Esto párrafo lo usé en intro de Capitulo 4). Los lenguajes combinatorios pueden describir un vasto conjunto de conceptos a partir de un pequeño conjunto de primitivas. Podemos entenderlo con un ejemplo relativamente simple en el dominio de las formas. Un lenguaje combinatorio similar a Logo~\cite{abelson1974logo} puede combinar símbolos de operaciones como ``mover", ``lápiz arriba", ``lápiz abajo" o ``rotar" para generar un conjunto infinito de expresiones (o programas) que, cuando se evalúan, pueden trasmitir todo tipo de formas.

%A language describing concepts (like shapes) also provides a natural notion of their complexity~\cite{kolmogorov1968three}. A concept is simple, relative to that language, when it can be described by a short program. On the contrary, it is complex when all its descriptions require a long sequence of instructions. For example, in the case of the Logo language, a square can simply be instructed as a loop of four displacements followed by rotations of 90 degrees. In this language, the icon of a face will be implemented by a significant lengthier program and hence will be more complex.  However, this concept would be simpler when described in a language in which the icon of a face (or the symbols for nose, mouth, etc.) are available as primitives in the language.

Un lenguaje que describe conceptos (como las formas) también proporciona una noción natural de su complejidad~\cite{kolmogorov1968three}. Un concepto es simple, relativo a ese lenguaje, cuando puede ser descripto por un programa corto. Por el contrario, es complejo cuando todas sus descripciones requieren una secuencia larga de instrucciones. Por ejemplo, en el caso de Logo, un cuadrado se puede codificar simplemente como un bucle de cuatro desplazamientos seguidos de rotaciones de 90 grados. En cambio, describir una cara requerirá de un programa más largo y, por lo tanto, será un concepto más complejo de describir. Sin embargo, este concepto se podría describir de manera más simple en un lenguaje en el que el icono de una cara (o de una nariz, boca, etc.) estuvieran disponibles como primitivas de ese lenguaje. 
}

\paragraph{Ubicar 4}

La adquisición de conceptos es un aspecto clave y ampliamente estudiado de la cognición diaria humana~\cite{cohen2005handbook, ashby2011human}. Muchos investigadores han afirmado que un sistema de codificación y un conjunto de reglas subyacen a algunas de nuestras habilidades para adquirir conceptos~\cite{nosofsky1994rule, tenenbaum2011grow, maddox1993comparing}, y se ha observado que parece que aprendemos conceptos de objetos con más facilidad cuando hay reglas `más simples' que pueden explicar esas agrupaciones~\cite{shepard1961learning, nosofsky1994comparing, rehder2005eyetracking, lewandowsky2011working, feldman2000minimization, blair2003easy, minda2001prototypes}.

\paragraph{Ubicar 3}

Por lo tanto, investigaciones previas indican que --como mínimo-- dos sistemas distintos pueden ser parte de la base del aprendizaje de secuencias en el cerebro humano: el aprendizaje estadístico y el aprendizaje basado en reglas~\cite{f4,f67,f85}. Lo que se desconoce es si operan de forma independiente y si alguno es favorecido por sobre el otro en función de la naturaleza de la información a codificar. Argumentamos que cualquier intento por descubrir los mecanismos cognitivos específicos detrás del aprendizaje de reglas en humanos, especialmente en comparación con otras especies, debe tener también en cuenta la contribución de un sistema menos abstracto (sin embargo poderoso) de predicción basado en las propiedades estadísticas de los eventos. \sergio{este párrafo es para una intro más general dado que plantea una de las contradicciones principales con las que pivotea LoT}\santi{Sí}


\textbf{Explicar críticas a simbólico} \cite{blackburn1984spreading, loewer1991meaning, knowles1998language, aydede1997language} \\


\section{Lenguaje del pensamiento}

\textbf{Profundizar Fodor y Cognición. Aparición de Probabilistic Language of Thought}\\

A pesar de las críticas y objeciones, los enfoques simbólicos en general --- y la hipótesis de LoT en particular --- han ganado una atención renovada con resultados recientes que podrían explicar el aprendizaje a través de diferentes dominios como inferencia estadística sobre un espacio de hipótesis estructurado composicionalmente \cite{tenenbaum2011grow, piantadosi2016four}.

El LoT no es necesariamente único. De hecho, la forma que adopta se ha modelado de muchas formas diferentes según el dominio del problema: aprendizaje de conceptos numéricos \cite{piantadosi2012bootstrapping}, aprendizaje de secuencias \cite{marie2016, yildirim2015learning, romano2013language}, aprendizaje de conceptos visuales \cite{ellis2015unsupervised}, aprendizaje de teorías \cite{ullman2012theory}, etc.

Si bien los marcos pueden diferir en cómo se puede implementar un LoT computacionalmente, todos comparten la propiedad de estar construidos a partir de un conjunto de símbolos y reglas atómicos mediante los cuales se pueden combinar para formar expresiones nuevas y más complejas. 

La mayoría de los estudios de LoT se han centrado en el aspecto compositivo del lenguaje, que se ha modelado dentro de un \cite{tenenbaum2011grow} bayesiano o un marco \cite{marie2016, goldsmith2002probabilistic, romano2013language, goldsmith2001unsupervised} de longitud mínima de descripción (MDL).

El método común es definir una gramática con un conjunto de producciones basadas en operaciones que son intuitivas para los investigadores y luego estudiar cómo diferentes procesos de inferencia coinciden con patrones regulares en el aprendizaje humano. Un estudio reciente \cite{piantadosi2016logical} pone el foco en el proceso de cómo elegir empíricamente el conjunto de producciones y cómo diferentes definiciones de LoT pueden crear diferentes patrones de aprendizaje. 

\paragraph{Otro Ubicar}

\intro{a intro}{
%Researchers have modeled these mental categories or conceptual classes with two classical approaches: in terms of similarity to a generic example or prototype~\cite{rosch1999principles,nosofsky1986attention,rosch1976structural,rosch1975family} or based on a symbolic/rule-like representation~\cite{boole1854investigation,fodor1975language,gentner1983structure}.
Los investigadores han modelado estas categorías mentales o clases conceptuales con dos enfoques clásicos: en términos de su similitud con un ejemplo genérico o prototipo~\cite{rosch1999principles,nosofsky1986attention,rosch1976structural,rosch1975family} o basados en una representación simbólica a través de reglas ~\cite{boole1854investigation,fodor1975language,gentner1983structure}.

%Symbolic approaches like the \textit{language of thought} (LoT) hypothesis~\cite{fodor1975language}, claim that thinking takes form in a sort of mental language, composed of a limited set of atomic symbols that can be combined to form more complex structures following combinatorial rules.

Enfoques simbólicos como la hipótesis del \textit{lenguaje del pensamiento} (LoT, por sus siglas en inglés)~\cite{fodor1975language}, afirman que el pensamiento toma forma en una especie de lenguaje mental compuesto por un conjunto limitado de símbolos atómicos que se pueden combinar para formar estructuras más complejas siguiendo reglas combinatorias.

%Despite criticisms and objections~\cite{blackburn1984spreading,loewer1991meaning,knowles1998language,aydede1997language}, symbolic approaches ---in general--- and the LoT hypothesis ---in particular--- have gained some renewed attention with recent results that might explain learning across different domains as statistical inference over a compositionally structured hypothesis space~\cite{tenenbaum2011grow,piantadosi2016four}.

A pesar de las críticas y objeciones~\cite{blackburn1984spreading,loewer1991meaning,knowles1998language,aydede1997language}, los enfoques simbólicos --en general-- y la hipótesis LoT --en particular-- han ganado una atención renovada con resultados recientes que podrían explicar el proceso de aprendizaje en diferentes dominios como un proceso de inferencia estadística sobre un espacio de hipótesis estructurado y componible~\cite{tenenbaum2011grow,piantadosi2016four}.

%The LoT is not necessarily unique. In fact, the form that it takes has been modeled in many different ways depending on the problem domain: numerical concept learning~\cite{piantadosi2012bootstrapping}, sequence learning~\cite{amalric2017language,yildirim2015learning,romano2013language}, visual concept learning~\cite{ellis2015unsupervised}, theory learning~\cite{ullman2012theory}, etc.

El LoT no es necesariamente único. De hecho, la forma que toma ha sido modelada de muchas formas diferentes dependiendo del dominio del problema: aprendizaje de conceptos numéricos~\cite{piantadosi2012bootstrapping}, aprendizaje de secuencias~\cite{amalric2017language,yildirim2015learning,romano2013language}, aprendizaje visual de conceptos~\cite{ellis2015unsupervised}, aprendizaje de teorías~\cite{ullman2012theory}, etc.

%While frameworks may differ on how a LoT may be implemented computationally, they all share the property of being built from a set of atomic symbols and rules by which they can be combined to form new and more complex expressions.

Si bien los trabajos pueden diferir en cómo se puede implementar un LoT computacionalmente, todos comparten la propiedad de estar construidos a partir de un conjunto de símbolos atómicos y reglas por las que se los pueden combinar para formar expresiones nuevas y más complejas.
}

\paragraph{Ubicar}

\intro{mandar a intro}{
Un enfoque más formal para la estimación de la complejidad de los patrones, conocido como \textit{complejidad algorítmica}, \textit{complejidad del tamaño del programa} o \textit{complejidad de Kolmogorov} (CK), fue propuesto por Kolmogorov~\cite{kolmogorov1965three}, Chaitin~\cite{chaitin1969length} y Solomonof~\cite{solomonoff1964formal}, en el marco de la \textit{teoría algorítmica de la información}. Estos matemáticos definieron a la complejidad de una secuencia como la longitud del programa computacional más corto capaz de producirlo. Estrictamente hablando, la complejidad algorítmica se define en relación un lenguaje descriptivo (o lenguaje de programación). Cuando este lenguaje es Turing completo, lo que significa que se puede simular cualquier otra máquina de Turing en él, hablamos de CK universal o CK simplemente. Sin embargo, cuando el lenguaje de codificación tiene un poder expresivo reducido (es decir, cuando es una máquina específica en lugar de una máquina universal), la complejidad algorítmica se puede calcular y utilizar como una medida subjetiva de complejidad~\cite{romano2013language}\sergio{nosotros}. Recientemente, se propuso una aproximación a CK utilizando el \textit{teorema de codificación}, que relaciona la complejidad algorítmica de una secuencia con la probabilidad de que una máquina universal produzca esa secuencia~\cite{f43,f44,f45,f46}. La propuesta proporciona una medida de complejidad algorítmica para un gran conjunto de secuencias cortas. Esta propuesta se presentó como la mejor aproximación de “una medida final de aleatoriedad” y puede reproducir los sesgos observados cuando se les pide a los individuos que juzguen la aleatoriedad de los patrones o que produzcan patrones aleatorios~\cite{f44,f45}.
}

Como se indicó anteriormente, una propuesta estrictamente relacionada con la CK es que los sujetos humanos comprimen las secuencias internamente, no necesariamente usando un conjunto de instrucciones de un lenguaje universal, sino usando una variedad de primitivas similares a las de una computadora tales como bucles y otras rutinas que forman un \textit{lenguaje del pensamiento} (LoT, por sus siglas en inglés) \santi{Repe} interno específico~\cite{fodor1975language} lo suficientemente fuerte para describir cualquier secuencia del dominio, pero no tan complejo como el de una Máquina de Turing universal y, por lo tanto, lo suficientemente débil como para permitir que la complejidad pueda ser computada de manera exacta. Dicho lenguaje permitiría la combinación de primitivas simples en patrones complejos de encajes o reglas recursivas. Los modelos de LoT se han propuesto muy temprano~\cite{f33}. Simon y Kotovsky~\cite{f48} utilizaron conceptos como ``igual'', ``siguiente'' (en el alfabeto) y la capacidad de recorrer una serie para construir una representación formal de la memoria humana para secuencias de letras (por ejemplo, ``cadaeafa...''). Del mismo modo, Restle~\cite{f37} utilizó las operaciones ``repetir'', ``transposición'' y ``reflexión''. También se utilizaron lenguajes similares basados en repeticiones con variaciones para codificar figuras geométricas lineales y formas 2D y 3D más elaboradas~\cite{f33,leeuwenberg1971perceptual}. Más recientemente, se han utilizado con éxito propuestas similares para estudiar diferentes aspectos del aprendizaje humano, en particular el aprendizaje de conceptos~\cite{feldman2000minimization,f51, piantadosi2012bootstrapping,piantadosi2016four,f54}. \santi{media tesis es sobre esto. Referenciar a los capítulos correspondientes} Se ha mostrado que la complejidad Booleana, es decir, la longitud de la expresión lógica más corta que captura el concepto (una noción estrechamente relacionada con CK) captura el comportamiento humano en el aprendizaje de conceptos lógicos~\cite{feldman2000minimization,feldman2003simplicity}.

\paragraph{Ubicar 2}

En pocas palabras, la suposición es que para minimizar la carga de memoria, los participantes comprimen mentalmente la estructura de la secuencia utilizando el lenguaje formal propuesto.\santi{Esta es la visión de Mariano, la que dice que usamos este lenguaje... Para mí, es simplemente que este lenguaje lo puede explicar y que nuestra cabeza podría estar usando otro. Tal vez valga la pena una pequeña digresión sobre esto en las conclusiones.} El uso de un número mínimo de operaciones primitivas y la selección de la representación más corta, están en sintonía con el principio de simplicidad (propuesto como un componente esencial del aprendizaje) el cual define que las hipótesis más simples son las más favorecidas~\cite{f32,feldman2003simplicity}.

\subsection{Gramáticas}


\textbf{Explicar gramáticas}\\
\textbf{Explicar diferencia entre sintaxis y semántica}\\

El proyecto de análisis bayesiano del aprendizaje de conceptos de modelos LoT utilizando inferencia bayesiana en un espacio de hipótesis estructurado gramaticalmente \cite{goodman2008rational}. Cada propuesta de LoT suele formalizarse mediante una gramática libre de contexto $ \gram $ que define las funciones o programas válidos que se pueden generar, como en cualquier otro lenguaje de programación. Un programa es un árbol de derivación de $ \gram $ que debe interpretarse o ejecutarse de acuerdo con una semántica determinada para obtener una descripción real del concepto en la tarea cognitiva en cuestión. Por lo tanto, cada concepto es luego representado por cualquiera de los programas que lo describen y se define un proceso de inferencia bayesiano para inferir de los datos observados la distribución de programas válidos en $ \gram $ que describen los conceptos.

\subsection{Composición}
Los lenguajes combinatorios pueden describir un vasto conjunto de conceptos a partir de un pequeño conjunto de primitivas. Esto se puede entender en un ejemplo relativamente simple en el dominio de las formas. Un lenguaje combinatorio y simbólico similar a Logo ~\cite{abelson1974logo} puede combinar operaciones como "mover", "pluma arriba", "pluma abajo" o "rotar" para generar un conjunto infinito de expresiones (o programas) que, cuando se evalúa, puede transmitir todo tipo de formas.

Un lenguaje que describe conceptos (como formas) también proporciona una noción natural de su complejidad \cite{kolmogorov1968three}. Un concepto es simple, relativo a ese lenguaje, cuando puede describirse mediante un programa corto. Por el contrario, es complejo cuando todas sus descripciones requieren una larga secuencia de instrucciones. Por ejemplo, en el caso del lenguaje Logo, un cuadrado puede simplemente instruirse como un bucle de cuatro desplazamientos seguidos de rotaciones de 90 grados. En este lenguaje, el icono de un rostro se implementará mediante un programa mucho más largo y, por lo tanto, será más complejo. Sin embargo, este concepto sería más sencillo cuando se describiera en un lenguaje en el que el icono de un rostro (o los símbolos de nariz, boca, etc.) estén disponibles como primitivos en el lenguaje.

En el dominio de los conceptos booleanos, se estudió una amplia gama de variedades lógicas de conceptos en ~\cite{feldman2003simplicity}, revelando una ley sorprendentemente simple: la dificultad subjetiva de un concepto booleano para un aprendiz humano es directamente proporcional a la longitud del programa compatible más corto en el lenguaje de la lógica proposicional (es decir, variables booleanas combinadas con los operadores \textit{and}, \textit {or} y \textit{not}). Este resultado puede sugerir que el LoT humano está equipado con reglas y símbolos similares a los que se encuentran en la lógica proposicional. De hecho, la correlación entre la dificultad subjetiva de los conceptos y su complejidad se ha utilizado como vehículo general para estudiar el LoT humano en varios dominios ~\cite{piantadosi2016logical, leeuwenberg1971perceptual, amalric2017language, romano2018, lupyan2007language}. Aunque a menudo está implícito, la estrategia general es (\textit{1)} asumir un idioma; (\textit{2)} encontrar el programa compatible más corto para algunos conceptos en ese idioma; (\textit{3)} comparar la duración de estos programas con la dificultad subjetiva de los conceptos; y finalmente (\textit{4)} repetir este proceso para varios idiomas dentro de un universo de posibles candidatos y elegir el idioma que mejor se ajuste en \textit{(3)}. Como se mencionó anteriormente, la longitud del programa depende de las primitivas del lenguaje en el que está escrito este programa, por lo que diferentes lenguajes hacen diferentes predicciones.

Una pregunta natural, sin embargo, es si las primitivas de una LoT son universales --tanto a través de diferentes individuos como a lo largo del desarrollo-- o si, en cambio, el repertorio semántico de un lenguaje es dinámico y está moldeado por la experiencia. De hecho, es probable que nuestra capacidad para representar automáticamente conceptos booleanos de manera sucinta no se deba a un lenguaje proposicional eficiente innato en nuestra mente. En cambio, proponemos que esta capacidad surge como un subproducto de nuestro cerebro que aprende rápidamente representaciones eficientes de los conceptos que generalmente encontramos en la vida cotidiana. 

Nuestra pregunta de investigación es: ¿con qué rapidez podemos adaptar nuestros mecanismos de aprendizaje cuando nos encontramos con un nuevo dominio en el que nuestras representaciones a priori ya no son eficientes? Examinamos la hipótesis de que los humanos tienen la capacidad de recombinar rápidamente proposiciones en su LoT, agregando nuevas primitivas a su lenguaje. En otras palabras, ese aprendizaje conduce a un proceso de compilación de rutinas en funciones dentro de el LoT.

En el ejemplo del lenguaje Logo se puede imaginar que si las producciones que dibujan cuadrados son muy frecuentes, sería eficaz dedicar un nuevo símbolo a esta producción. El nuevo símbolo "cuadrado" es una construcción jerárquica de "segundo orden" de las primitivas de "primer orden" del lenguaje. Tiene un costo (de incrementar el léxico del lenguaje) pero en el nuevo lenguaje, dibujar un cuadrado puede ser instanciado con un programa muy corto (es decir, "cuadrado") y por lo tanto usa menos memoria. De hecho, un lenguaje de nivel superior nos permite alcanzar un nivel superior de abstracción al liberar la memoria y el poder de procesamiento, haciendo así pensables pensamientos más complejos ~\cite{minsky1967computation, murphy1988comprehending}.

La mayor parte del trabajo en la literatura sobre LoT, aunque incluye naturalmente un mecanismo de aprendizaje, tiende a acercarse al LoT como un sistema estable que deben descubrir los experimentadores, que prueban diferentes plantillas candidatas y seleccionan la que mejor se ajusta a los datos después del entrenamiento ~\cite{goodman2008rational, kemp2012exploring, piantadosi2016logical}. Aún así, queda por descubrir cómo las diferentes trayectorias de la experiencia pueden dar forma a la adquisición de manera diferente y pueden cambiar constantemente el repertorio de un LoT después de cada exposición.


\section{Teoría algorítmica de la información}

\widesanti{De esto que sigue podés dejar todo, algo o eliminarlo por completo}

Cuando vemos las cadenas de 26 bits
\begin{align*}
\sigma_1 &= 10101010101010101010101010,\\
\sigma_2 &= 00100100001111110110101010, \\
\sigma_3 &= 11011010010000110101111010,
\end{align*}
tenemos la sensación de que la $\sigma_2$ y la $\sigma_3$
son {\em más difíciles} o {\em más complejas} que $\sigma_1$. Sin embargo, en términos de
teoría de la medida, cada una es igual de probable, si provienen de una fuente que emite bits 
con probabilidad uniforme. 
La intuición es que las cadenas sencillas tienen patrones reconocibles (como $\sigma_1$) mientras
que las más complejas no (o al menos no tienen patrones simples). 
En efecto, con la información adicional de que $\sigma_2$ se compone de los dígitos de 
la expansión fraccionaria de $\pi$ en binario, esta secuencia --intuitivamente-- se vuelve de inmediato más simple que antes, puesto que aparece una regla para producirla. Lo cierto es que $\sigma_3$ fue generada tirando una moneda y anotando 0 si salía cara y 1 si salía ceca. Nuestra intuición también nos dice que lo más probable es que una cadena generada mediante este procedimiento no tenga patrones reconocibles.

Entender por qué
algunas secuencias parecen aleatorias o complejas mientras que otras no, es una cuestión
profunda y fundamental. Desde principios del siglo {\small XX} se
empezó a pensar en este problema. Pero cómo transformar nuestra
intuición de `lo complejo' en una definición matemática, y cómo calibrar esta noción, no fue una tarea fácil. 
Al final, las definiciones que se
encontraron terminaron estando todas muy relacionadas con los métodos efectivos y con la 
teoría de la computación.

% La {\em teoría de largo de programa} define una noción de
% complejidad que clasifica las cadenas de 0s y 1s según la longitud
% del programa más corto que las computa. Esta complejidad se llama
% {\em complejidad de Kolmogorov}. Las cadenas más complejas son
% aquellas que requieren un programa esencialmente de igual longitud
% que la cadena misma, y las más sencillas son las que admiten un
% programa sustancialmente más corto. 

La {\em teoría algorítmica de la información}, también
conocida como {\em teoría de largo de programa} fue iniciada
independientemente en la década del 60 por Kolmogorov \cite{kolmogorov1965three},
Solomonoff \cite{solomonoff1964formal} y Chaitin \cite{chaitin1969length}. 
Esta teoría define una
noción de complejidad de cada palabra teniendo en cuenta la longitud
del programa más corto que computa esa palabra. Así, los programas
son vistos como {\em descripciones algorítmicas} de palabras. De
entre todas las descripciones de una palabra podemos tomar la que
tiene menor longitud como una medida de su complejidad. Una palabra
$\sigma$ es simple, es decir, tiene baja complejidad, si su
complejidad es sustancialmente menor que la longitud de $\sigma$; y
una palabra es compleja si su descripción algorítmica es tan larga
como la longitud misma de $\sigma$. La gran mayoría
de las palabras tiene complejidad alta. La función que asocia a cada
palabra $\sigma$ la longitud del programa más corto que devuelve
$\sigma$ como salida (cuando es ejecutado en una cierta máquina
universal de referencia) se llama {\em complejidad de Kolmogorov} o
{\em complejidad de largo de programa}.

Debe
observarse que en esta definición --todavía informal-- 
de complejidad de Kolmogorov hay
un lenguaje de descripción subyacente. La teoría algorítmica de la información
toma a los lenguajes de programación como lenguaje de descripción. Sin embargo, 
hay que notar que una definición en la misma dirección puede aplicarse para 
otros objetos en el rol de las cadenas binarias (por ejemplo, conjuntos de valuaciones 
Booleanas) y otros lenguajes de descripción en el rol de lenguajes de programación
(por ejemplo, fórmulas de la lógica proposicional). En todo caso, es fundamental
que los lenguajes de descripción tengan una semántica formal para evitar paradojas
como la de Berry, que propone definir $n$ como el primer número natural que no sea 
descriptible con menos de 90 caracteres. Por definición la descripción más corta de $n$ debe tener 
al menos 91 letras, y sin embargo su definición ``el primer número natural que no sea 
descriptible con menos de cincuenta caracteres'' evidentemente lo describe y tiene 83 caracteres.

Desde el punto de 
vista teórico, la riqueza de la Complejidad de Kolmogorov
aparece cuando el lenguaje de programación subyacente es 
Turing completo, 
es decir, es capaz de definir cualquier función parcial
computable. En la teoría, los lenguajes de programación se
formalizan con máquinas de Turing y los lenguajes Turing completos
como máquinas universales. 
Cuando el dominio de los programas (máquinas) es libre de prefijos (es decir, no hay dos programas
que terminan en dónde uno es una extensión propia del otro)
hablamos de complejidad de Kolmogorov {\em libre de prefijos}
(denotada con $K$) y cuando no se impone ninguna restricción en
el dominio, hablamos de complejidad de Kolmogorov plana (denotada
con $C$). La necesidad de trabajar con dominios libres de prefijos es
técnica y está vinculada con la utilidad de la complejidad de Kolmogorov 
para definir nociones de aleatoriedad.
% Esta noción de complejidad es absoluta, pues si bien existe una
% complejidad distinta por cada máquina universal subyacente, todas
% estas son iguales salvo una constante aditiva \cite{C94,li2013introduction}.

A continuación veremos las definiciones formales de la Complejidad de Kolmogorov y de los
principales elementos de la Teoría algorítmica de la información.

% Los orígenes del estudio de la aleatoriedad algorítmica se remontan
% a los trabajos de von Mises de principios del siglo {\small XX}
% \cite{M19}, en dónde argumentaba que las secuencias aleatorias deben
% tener ciertas propiedades de estocasticidad desde el punto de vista
% de la teoría clásica de la probabilidad.

% Intuitivamente una secuencia es aleatoria cuando carece de
% estructura o regularidad, en otras palabras, cuando no tiene
% patrones reconocibles. Uno querría definir a las secuencias
% aleatorias como aquellas que  son indistinguibles del resultado de
% arrojar infinitas veces una moneda y anotar $0$ si sale cara o $1$
% si sale ceca. Así, las secuencias
% $$
% 00000000000000000000\dots \mbox{\quad o \quad }
% 10101010101010101010\dots
% $$
% no parecen aleatorias porque se pueden reconocer marcados patrones.
% En cambio, uno siente que una secuencia como
% $$
% 10010111010111100101\dots
% $$
% es aleatoria porque es difícil reconocer patrones. Las
% clarificaciones de precisamente qué constituye lo aleatorio fueron
% hechas recién pasada la mitad del siglo {\small XX}.

% Martin-L\"of \cite{M66} introdujo una noción de aleatoriedad basado
% en tests estadísticos. La idea es que una secuencia aleatoria
% debería pasar todo posible test estadístico razonable. Martin-L\"of
% formaliza esta noción de {\em test estadístico razonable} como un
% tipo particular de conjuntos efectivos de medida $0$. Su propuesta
% es que una secuencia es aleatoria cuando evita (es decir, logra
% escapar de) todos esos conjuntos efectivos de medida $0$.

% Independientemente, Chaitin \cite{C76b} introdujo la noción de
% aleatoriedad en términos de la complejidad de Kolmogorov (en
% realidad, de una variante de esa complejidad que se denomina
% complejidad {\em libre de prefijos}): las secuencias
% Chaitin-aleatorias son aquellas cuyos segmentos iniciales son {\em
% incompresibles}. Es decir, la complejidad de Kolmogorov de los
% primeros $n$ dígitos de la secuencia es mayor que $n$ menos una
% constante fija.


% Es interesante el hecho de que la idea de {\em procedimiento
% efectivo} está involucrada en todas las definiciones de aleatoriedad. 

\subsection{Complejidad de Kolmogorov} \label{INTRO:KOLMOGOROV}
\widesanti{Introducir la noción formal. Empezar por $K_M$, con $M$ una máquina específica. Luego la $K_U$, con $U$ universal. Dar la versión plana y la libre de prefijos (puesto que esa es la que se usa para el coding theorem). Dar el coding theorem sin demo, solo enunciado.}


\widesanti{Podés retomar los ejemplos de $\sigma_1$, $\sigma_2$, $\sigma_3$ de más arriba y explicar que programas cortos los generan. O podés exagerarlos a más bits. Esto da idea de la diferencia entre la descripción y lo descripto (aunque es cierto que la tesis es sobre aprendizaje con POCOS datos...) Para los programas, alcanza con un pseudo código o incluso descripción informal, pero dado que es una tesis en computación, también podés usar un lenguaje verdadero, Python, ponele. Podés también poner alguna otra cadena, $\sigma_4$ en donde haga falta anidar loops, porque eso es importante para el cap 2. Por ejemplo, 000111000 1 000111000 1 000111000}


\widesanti{Esto que sigue es parte de mi tesis. Podés simplemente traducirlo y dejarlo tal cual. 
Ojo que hay cosas de más... y muchos comandos latex :P}


In this section we introduce the plain and \pfree \kolcomp and we
mention some known results and standard notation that will be used
repeatedly in the rest of this thesis. Some other important
results will be introduced when necessary.

In algorithmic information theory, programs are regarded as
algorithmic descriptions of strings. As we mentioned in
section~\ref{intro:sec:recursion}, we say that a program $\pra$
{\em $\M$-describes} a string $\sta$ when it is executed in a
Turing machine $\M$ and produces $\sta$ as output. In general
there is more than one program describing the same output $\sta$.
The idea of the \kolcomp introduced in~\cite{kolmogorov1965three,solomonoff1964formal,chaitin1975theory} is to
take the length of a shortest description as a measure of the
complexity of the string.

\begin{definicion}[Plain \kolcomp]\label{intro:def:plainC}
$\CM\colon\words \to \nat$, 
the {\em plain
\kolcomp with respect to the Turing machine $\M$}, is defined as
$$
\CM(\sta)=
    \begin{cases}
    \min \{\len{\pra}\colon \M(\pra)=\sta \} & \textrm{if $\sta$ is in the range of $\M$};\\
    \infty & \textrm{otherwise.}
    \end{cases}
$$
\end{definicion}




\widesanti{acá hablar solo de la complejidad plana. Decir que la complejidad relativa a una máquina universal es la más chica entre todas las complejidades (salvo cte aditiva). Decir que para algunas máquinas particulares la complejidad es una función computable y dar un ejemplo simple - podria ser una máquina que dado n representando un número en binario, devuelve 0000... n veces; o una máquina que sencillamente copia la entrada en la salida. Decir algo sobre el counting como está acá 

https://www.dropbox.com/s/w95pz1euie8a39r/Li-Vitanyi.pdf?dl=0

en Def 2.2.1 y párrafo siguiente (pag 116)

Tal vez podemos decir que la def de Kolmogorov complexity induce la idea de MDL. No sé como lo tenías pensado... veo que hay un cap sobre MDL. En algún lugar de la intro, hacer la conexión entre nuestras definiciones de MDL y la def de Kolm compl (eg para definir la la compl kolm relativa a la máquina $M$, ($C_M$) la semántica de un programa $p$ es el resultado de su ejecución en  $M$, $\sem{p}_M=M(p)$)


No hablar nada de prefix machines o prefix complexity en la intro. Sacar lo de Craft Chaitin y lo de oráculos.

Pasar la def. de prefix machines, prefix complexity y coding thm al cap 3
}


\begin{definicion}[\Optity]\label{intro:def:optimal}
\index{Turing!machine!universal@\opt} $\U$ is a {\em \opt} \pfree
(resp.\ classical) Turing machine if and only if
$$
(\forall e)(\exists c_e)(\forall \pra)(\exists \prb_{e,\pra})\,
[\U(\prb_{e,\pra})=\T_e(\pra) \ \wedge \len{\prb_{e,\pra}}\leq
\len{\pra}+c_e],
$$
where $\T_0,\T_1,\T_2,\dots$ is an enumeration of all the \pfree
(resp.\ classical) Turing machines
\end{definicion}

When working with oracles, $\U$ will be \opt if it may simulate
{\em any} other machine with {\em any} oracle, so it is {\em
universally universal}. In the case of an \pfree \opt machine,
$\U^A$ will be \pfree for all $A\subseteq\nat$.

In general we use the same letter $\U$ for denoting both a
classical \opt machine and a \pfree \opt machine; it will always
be clear from the context which one we refer to. In~\cite{chaitin1975theory}
Chaitin called these machines {\em optimal universal}, because
they permit us to define the \kolcomp in an {\em optimal} way:

\widesergio{Def1, 3 y 4 pero con las maquinas comunes nada prefijos. Y pasar la 2 y teorema de codificacion. Buscar si Counting aplica a complejidad comun. Si es así quedan, sino salen. Sacar definicion 5. Mover Coding Teorem y sacar definicion 7 que ya no aplica}

\begin{teorema}[Invariance]\label{intro:thm:invariance}
\index{Invariance Theorem} If $\U$ is \opt \pfree (resp.\
classical) Turing machine then for any \pfree (resp.\ classical)
Turing machine $\M$ there is a constant $c$ such that for all
$\sta\in\words$, $\KU(\sta)\leq \KM(\sta)+c$ (resp.\
$\CU(\sta)\leq \CM(\sta)+c$).
\end{teorema}

Hence for any two \pfree \opt machines $\U$ and $\V$, the \pfree
\kolcomp with respect to $\U$ and $\V$ is the same up to an
additive constant, i.e.\ there is some constant $c$ such that
$\abs{\KU(\sta)-\KV(\sta)}\leq c$ for any $\sta\in\words$. Of
course the same is true for classical machines and $\C$. If there
is no need to refer to the underlying \opt machine $\U$, one just
writes \glossary{$\K$}$\K$ for $\KU$ and \glossary{$\C$}$\C$ for
$\CU$. In this way, $\K$ and $\C$ become an {\em absolute} measure
of the complexity of the strings.

To illustrate some useful application of the Invariance Theorem,
let us see some upper bounds for $\K$ and $\C$. Imagine a
classical machine $\M$ that reads the input $\sta$ and writes
$\sta$ as its own output. Then $\CM(\sta)=\len{\sta}$ for all
$\sta\in\words$ and by the Invariance
Theorem~\ref{intro:thm:invariance} this shows that for all $\sta$,
$\C(\sta)\leq \len{\sta}+c$ for some constant $c$. With \pfree
machines the situation is a bit different because the machine has
to discover by itself where the end of the input is. Suppose $\U$
is any \opt \pfree machine and suppose $\pra_\sta$ is a minimal
$\U$-description of $\len{\sta}$. This just means that
$\U(\pra_\sta)=\len{\sta}$ and $\len{\pra_\sta}=\K(\len{\sta})$.
Then there is a \pfree machine $\N$ that with input
$\pra_\sta\sta$ can do the following: fist simulate $\U$ step by
step. Each time $\U$ asks for reading one more bit, $\N$ obeys and
reads from its own input. Eventually $\U$ will read the whole
$\pra_\sta$ and will terminate with $\U(\pra_\sta)=\len{\sta}$ as
output. Now $\N$ (which is still running) knows that exactly
$\len{\sta}$ more bits need to be read from the input. So $\N$
reads $\len{\sta}$ bits from the input and recovers $\sta$.
Afterwards, $\N$ writes $\sta$ in the output and terminates. This
shows that $\N(\pra_\sta\sta)=\sta$. Of course, if the input is
wrong, the computation may go wrong, for example $\N$ can try to
read beyond the end of the input and then crashes. However, when
the input is of the form $\pra_\sta\sta$, $\N$ always outputs
$\sta$. By the invariance Theorem \ref{intro:thm:invariance} this
shows that $\K(\sta)\leq \len{\pra_\sta\sta} + d =
\K(\len{\sta})+\len{\sta}+d$ for some constant $d$. These upper
bounds on $\C$ and $\K$ will be repeatedly used in the thesis. We
will also use that for any $n$ there is a string $\sta$ of length
$n$ such that $\K(\sta)\geq n$. This is true because there are at
most $2^n-1$ programs of length less than $n$ but $2^n$ strings of
length $n$. The same holds for $\C$. In fact,
Chaitin~\cite{chaitin1975theory,C87b} showed that there is a constant $c$ such
that for all $d\in\nat$ and all $n$
$$
\size{\{ \sta \colon \len{\sta}=n \wedge \K(\sta)\leq n + \K(n)-d
\}} \leq 2^{n-d+c}
$$
This result is usually called \index{Counting Theorem}Counting\santi{estos resultados de 'counting' pueden ser interesantes porque justifican algo que decimos en el cap 2. Buscar mi comentario con ***}
Theorem, tells us that only a small fraction of all the strings of
length $n$ have \pfree \kolcomp below $n+\K(n)-d$, when we take
$d$ much bigger than $c$.
Observe that if we let $d=n-b$, we obtain
$$
\size{\{ \sta \colon \len{\sta}=n \wedge \K(\sta)\leq \K(n)+b
\}} \leq 2^{b+c}
$$
and this last upper bound depends on $b$ and $c$, but not on $n$.

As we explained in the last paragraph, one way to bound the
\kolcomp is to explicitly design specific machines, like $\M$ or
$\N$ and explain their behavior. Another powerful method to
implicitly build \pfree machines and upper bound the \pfree
\kolcomp is via a Kraft-Chaitin set. This method consists of an
effective interpretation of an inequality of Kraft~\cite{K49}:

%\begin{definicion}[Kraft-Chaitin set]\label{intro:KCset}
%\index{Kraft-Chaitin} A \ce\ set
%$$
%W=\{\pair{n_0}{\sta_0}, \pair{n_1}{\sta_1},
%\pair{n_2}{\sta_2},\dots\},
%$$
%where $n_i\in\nat$ and $\sta_i\in \words$, is a {\em %Kraft-Chaitin
%set} if $\wt{W}\leq 1$, where \glossary{$\wt{W}$}$\wt{W}=
%\sum_{i\in\nat} 2^{-n_i}$ is the {\em weight} of $W$. The %pairs
%enumerated into such a set $W$ are called {\em axioms}.
%\end{definicion}

%The Kraft-Chaitin Theorem can be found in Levin's
%Thesis~\cite{L71} and in~\cite{levin1974laws}, Schnorr also included a
%version of it in~\cite{S73} and Chaitin~\cite{chaitin1975theory} gave the first
%proof explicitly for \pfree \kolcomp. This theorem states that
%from a Kraft-Chaitin set $W$ like the one described in the above
%Definition~\ref{intro:KCset}, we can effectively obtain a \pfree
%machine $\M$ such that for each $i$ there is a $\pra_i$ of length
%$n_i$ with $\M(\pra_i)\downarrow=\sta_i$, and $\M(\prb)\uparrow$
%unless $\prb=\pra_i$ for some~$i$. Observe that the machine $\M$
%is built in an implicit way: we only need to specify the lengths
%of the programs we want. In particular, the Kraft-Chaitin Theorem
%states that there is a constant $c$ such that for all $i$,
%$\K(\sta_i)\leq\min\{m\colon\pair{m}{\sta_i}\in W\}+c$.

\bigskip


\subsubsection{Inducción de Solomonoff}
\widesanti{Algo corto. Está bien explicado en el Vitanyi}

\subsubsection{MDL}
\widesanti{Acá hay varios trabajos para referenciar. Se puede mover parte de la intro de capítulo 2. Sería bueno poner la descripción del lenguaje de geometría, como ejemplo de lenguaje específico no Turing completo que describe secuencias espaciales. Estaría bueno dar la semántica formal (yo la tenía en algún lado), puesto que es una tesis en computación. Esta misma semántica se vuelve a dar en el cap 2, pero simplificada, dado que son solo 2 puntos.} 

\subsection{Ciencia Cognitiva Bayesiana}
\label{INTRO:BAYES}
\subsubsection{Rational analysis y plot}

Aunque el estudio actual se basa en el uso de un lenguaje fijo, con reglas y pesos asociados, alguna evidencia sugiere que una mejor descripción del comportamiento se puede lograr incorporando un componente probabilístico al modelado. Este enfoque, defendido por~\cite{piantadosi2016four} bajo el término lenguaje de pensamiento probabilístico (pLoT), consiste en utilizar inferencia probabilística bayesiana para estimar la probabilidad de la existencia de algún conjunto de reglas (un lenguaje formal propuesto), dada los datos observados. Se ha demostrado que es especialmente eficaz para modelar el aprendizaje de conceptos,por ejemplo, replicando los patrones de errores a lo largo del aprendizaje~\cite{goodman2008rational,piantadosi2012bootstrapping,piantadosi2016logical}. Este enfoque también se adoptó para investigar cómo los seres humanos evalúan la aleatoriedad en su entorno. Los sesgos humanos en los juicios subjetivos de aleatoriedad~\cite{f114,f115} podrían explicarse asumiendo que la representación de la aleatoriedad resulta de una inferencia estadística sobre los procesos que generaron la secuencia, es decir, una estimación de la probabilidad de que un proceso regular dado lo produjo~\cite{f21}. Un buen ajuste al comportamiento humano fue obtenido sin utilizar toda la potencia de las máquinas de Turing, sino simplemente autómatas de estado finito con una pila, que son capaces de reconocer la repetición, la alternancia o la simetría~\cite{f18,f117}. Por lo tanto, a pesar de las diferencias fundamentales (en particular, los lenguajes deterministas versus probabilísticos), la teoría pLOT comparte con nuestro enfoque la necesidad de considerar tipos similares de operaciones primitivas. Dados los fuertes vínculos entre aleatoriedad y complejidad subjetivas, podemos esperar razonablemente que nuestro lenguaje formal también puede predecir si un patrón se percibe como aleatorio o no; esta posibilidad permanece para ser probado en trabajos futuros.

Como se mencionó anteriormente, el enfoque pLOT permite encontrar los conceptos y reglas más probables en un espacio de hipótesis estructurado gramaticalmente que contiene varios candidatos utilizando la inferencia bayesiana, parece ser un enfoque muy prometedor para ese propósito~\cite{goodman2008rational,piantadosi2016four,romano2018bayesian}. 
